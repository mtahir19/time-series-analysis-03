{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Modeling\n",
    "\n",
    "> Authors: Matt Brems, Joseph Nelson, Justin Pounders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install scipy statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# We are required to do this in order to avoid \"FutureWarning\" issues.\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/global_land_temps.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Temperature Data\n",
    "> We have [average temperature data](https://data.world/data-society/global-climate-change-data) for the globe from 1750 to 2015.\n",
    "- `date`: the month the data was measured.\n",
    "- `avg_temp`: the average global temperature (in degreed Celsius)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set index to be the date column as a DatetimeIndex.\n",
    "df.set_index()\n",
    "\n",
    "# Drop the date column from our data.\n",
    "df.drop()\n",
    "\n",
    "# Confirm.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified from code written by Matthew Garton.\n",
    "\n",
    "def plot_series(df, cols=None, title='Title', xlab=None, ylab=None, steps=1):\n",
    "    \n",
    "    # Set figure size to be (18, 9).\n",
    "    plt.figure(figsize=(18,9))\n",
    "    \n",
    "    # Iterate through each column name.\n",
    "    for col in cols:\n",
    "            \n",
    "        # Generate a line plot of the column name.\n",
    "        # You only have to specify Y, since our\n",
    "        # index will be a datetime index.\n",
    "        plt.plot(df[col])\n",
    "        \n",
    "    # Generate title and labels.\n",
    "    plt.title(title, fontsize=26)\n",
    "    plt.xlabel(xlab, fontsize=20)\n",
    "    plt.ylabel(ylab, fontsize=20)\n",
    "    \n",
    "    # Enlarge tick marks.\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xticks(df.index[0::steps], fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a time plot of our data.\n",
    "plot_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What Pandas function may be helpful here?</summary>\n",
    "\n",
    "- We'll use `.resample()`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a time plot of our data.\n",
    "plot_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite df.\n",
    "df ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: How would you describe this data?</summary>\n",
    "    \n",
    "- **Trend**: The mean is increasing over time and it looks like it speeds up toward the end of the 1960s.\n",
    "- **Seasonality**: There are fluctuations, but it's tough to see if there are fluctuations on a fixed and known frequency.\n",
    "- **Autocorrelation**: The data are correlated with one another.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Model\n",
    "\n",
    "ARIMA stands for AutoRegressive Integrated Moving Average, and as the name suggests it's composed of three parts (and thus has three parameters): Autoregression, Moving Average, and the integration.\n",
    "\n",
    "An **autoregressive model** (AR) take a parameter $p$, which is the number of prior timesteps to include. It's effectively the number of lags that we see in our ACF.\n",
    "\n",
    "**Moving average** (MA) models take previous _error terms_ as inputs. They predict the next value based on deviations from previous predictions. This can be useful for modeling a sudden occurrence â€” e.g., something going out of stock affecting sales or a sudden rise in popularity.\n",
    "\n",
    "As with autoregressive models, moving average models have an order term, $q$, and we refer to our model as MA(q). This moving average model is dependent on the last q errors. If we have a time series of sales per week, $y_i$, we can regress each $y_i$ on the last q error terms.\n",
    "\n",
    "Combining AR and MA gives us an ARMA model, with parameters $p$ and $q$. This model predicts the value of a series. We won't discuss ARMA models, as they tend to be counter productive in time series analysis, and instead we'll move straight on to ARIMA, which integrates both AR and MA using differencing.\n",
    "\n",
    "In order to fit an ARIMA model, we need to specify all three parameters: $p$, $d$, and $q$.\n",
    "- $p$ in our autoregressive models (AR) is the number of timesteps to correlate with\n",
    "- $q$ in our moving average (MA) models is the number of prior error terms to regress on\n",
    "- $d$ is how much to difference our model\n",
    "\n",
    "## How to Choose the Right `p` and `q` Parameters\n",
    "---\n",
    "\n",
    "In general, it's never a bad idea to choose your parameters based on hold-out testing; that is to say, checking the performance of your model on future time points based on different choices of `p` and `q` for an ARIMA model.\n",
    "\n",
    "However, you can get a sense for which parameters will work best based on autocorrelation and partial autocorrelation charts.\n",
    "\n",
    "[This site](https://people.duke.edu/~rnau/411arim3.htm) offers a detailed overview of how to use ACF and PACF to determine your parameters.\n",
    "\n",
    "Below are some basic general guidelines. Remember that these rules apply to the ACF and PACF plots of differenced time series rather than the original time series (the exception being if your time series is stationary and does not require differencing):\n",
    "\n",
    "1. If the PACF has a sharp cut-off and the lag-1 ACF value is positive, choose an `AR(x)` term where `x` is the lag in the PACF after the cut-off.\n",
    "2. If the ACF has a sharp cut-off and the lag-1 ACF value is negative, choose an `MA(x)` term where `x` is the lag in the ACF after the cut-off.\n",
    "3. If both the ACF and PACF show a gradual decay, an ARMA model is likely appropriate as opposed to AR or MA alone.\n",
    "\n",
    "Context 1 above corresponds to time series that are \"underdifferenced,\" as indicated by a positive autocorrelation at lat 1. Likewise, Context 2 is \"overdifferenced,\" as indicated by a negative autocorrelation.\n",
    "\n",
    "In general, you should try to choose an AR or MA model as opposed to an ARMA model. The AR and MA terms can work against each other in the model and create an overly complex representation.\n",
    "\n",
    "### Parameter $d$: How much do we difference?\n",
    "\n",
    "$d$ is known as the **differencing parameter**. This controls how much we \"difference\" our time series by. _(We difference to meet an assumption - we'll talk about this in a minute!)_\n",
    "\n",
    "Differencing our time series means that instead of fitting a model that predicts our time series $Y_t$ directly, we'll fit a model to $Y'_t$ or $Y''_t$:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y'_t &=& Y_t - Y_{t-1} \\\\\n",
    "Y''_t &=& Y'_t - Y'_{t-1} = Y_t - 2Y_{t-1} + Y_{t-2}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "So if I wanted to fit a model, my \"new\" output variable I'm trying to predict would be $Y'_t$ or $Y''_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 values of our series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 values of our series, differenced once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 values of our series, differenced twice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first_diff_temp and second_diff_temp\n",
    "# columns in df.\n",
    "df['first_diff_temp'] =\n",
    "df['second_diff_temp'] ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why would we difference?](https://otexts.com/fpp2/stationarity.html) Well, there is one assumption that is **required** for nearly every time series model: **stationarity**.\n",
    "- If our time series is stationary, then we do not need to difference and let $d=0$.\n",
    "- If our time series is not stationary, then we difference either once ($d=1$) or twice ($d=2$). Differenced data often is stationary, so we difference our data, then model that!\n",
    "\n",
    "#### What is stationarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate 200 steps in time from 0 to 15.\n",
    "t = np.linspace(0, 15, 200)\n",
    "\n",
    "# Generate a white noise process with Normally distributed noise.\n",
    "y = 5 * np.random.normal(0, 2, 200)\n",
    "\n",
    "# Generate plot.\n",
    "plt.title('A stationary time series!', fontsize=15)\n",
    "plt.plot(t, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Informally, stationarity means that there **aren't systematic changes in our time series over time**.\n",
    "- Our mean stays the same. (There is no trend.)\n",
    "- The autocorrelation between $Y_t$ and $Y_{t-k}$ depends only on the size of our lag $k$. (There is no seasonality.)\n",
    "- A [white noise process](https://stats.stackexchange.com/questions/7070/what-is-a-white-noise-process) is a common example of a stationary time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Is our data stationary?</summary>\n",
    "    \n",
    "- No! The mean is not constant over time. It looks like the mean increases over time.\n",
    "- It looks like the fluctuations are more spread out in early years and get closer moving into later years.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature data.\n",
    "plot_series(df,\n",
    "            ['avg_temp'],\n",
    "            title = \"Average Temperature (in Celsius)\",\n",
    "            steps = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, why do we difference? \n",
    "- Differencing allows us to get a stationary time series out of a non-stationary time series.\n",
    "- This means that we'll be able to fit an ARIMA model to the **differenced** data!\n",
    "\n",
    "We'll start with a difference of $d = 1$ and iterate upward until we find a value of $d$ that makes our time series stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine temperature, differenced once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine temperature, differenced twice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if we let $d=1$ or $d=2$?\n",
    "\n",
    "#### Checking for Stationarity: the Augmented Dickey-Fuller Test\n",
    "\n",
    "The [augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test) is a hypothesis test that tests for stationarity. We assume that our data are not stationary. With enough evidence, we may accept that our data are stationary.\n",
    "\n",
    "Specifically, the test is as follows:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "&H_0:& \\text{not stationary} \\\\\n",
    "&H_A:& \\text{stationary}\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Would a small $p$-value or a large $p$-value give us evidence that our time series is stationary?</summary>\n",
    "\n",
    "- A small $p$-value would give us evidence to reject the null hypothesis, meaning we accept that our time series is stationary.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Augmented Dickey-Fuller test.\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Run ADF test on original (non-differenced!) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code written by Joseph Nelson.\n",
    "\n",
    "def interpret_dftest(dftest):\n",
    "    dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n",
    "    return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run ADF test on original (non-differenced!) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>At alpha of 0.01, how would we interpret this $p$-value? </summary>\n",
    "    \n",
    "- Remember that we compare our $p$-value to $\\alpha$, which is usually set at 0.10, 0.05, or 0.01. We'll use $\\alpha = 0.01$. Because $p = 0.0498$, it is larger than $\\alpha$. Thus, we cannot accept that our series `avg_temp` is stationary.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In order to achieve stationarity, we need to difference!\n",
    "\n",
    "We'll start with a difference of $d = 1$ to see if the time series `first_diff_temp` is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ADF test on our once-differenced data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Are our data, after a first-order difference, stationary?</summary>\n",
    "\n",
    "- Yes! This is stationary.\n",
    "- With a $p$-value of well below 0.05, we would reject $H_0$ and thus **accept that our first-order differenced data are stationary**.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rather than modeling $Y_t$ directly, we'll model $Y'_t = Y_t - Y_{t-1}$.\n",
    "- If we generate plots moving forward, then we should generate plots of the **differenced** time series, not the original time series!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit an ARIMA model.\n",
    "\n",
    "There are three hyperparameters we need to get values for when fitting an ARIMA model.\n",
    "1. Determine a value of $d$ using the Augmented Dickey-Fuller test.\n",
    "2. Then, determine values of $p$ and $q$ through GridSearching.\n",
    "\n",
    "An $ARIMA(p, d, q)$ model is specified by:\n",
    "- how many differences $d$ we need to calculate in order to achieve stationarity.\n",
    "- how many lags $p$ we regress $Y_t^{(d)}$ on.\n",
    "- how many errors $q$ we regress $Y_t^{(d)}$ on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Check: Recall that $AR$ incorporates information from long-term trends and $MA$ incorporates information from sudden shocks. What are some examples where we might see both long-term trends and sudden fluctuations in our time series data? </summary>\n",
    "\n",
    "- Stock price data. Stocks increase and decrease over time, but news or other stock changes may have sudden effects on prices. (Similar logic applies to gas or oil prices.)\n",
    "- Public transportation ridership. While public transportation may see slow changes over time, marketing campaigns, changes in price, or accidents may have a sudden, unforeseen shock on ridership.\n",
    "- Temperature data over time!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our values of $p$, $d$, and $q$, we might refer to these models by slightly different names.\n",
    "\n",
    "- If $d=0$ and $q=0$, an AR(p) model is specified by how many lags $p$ we regress $Y_t$ on.\n",
    "- If $d=0$ and $p=0$, an MA(q) model is specified by how many errors $q$ we regress $Y_t$ on.\n",
    "- If $d=0$, an ARMA(p, q) model is specified by how many lags $p$ and how many errors $q$ we regress $Y_t$ on.\n",
    "\n",
    "| p | d | q |          Model         |\n",
    "|:-:|:-:|:-:|:----------------------:|\n",
    "| 1 | 0 | 0 |  ARIMA(1,0,0) = AR(1)  |\n",
    "| 0 | 0 | 1 |  ARIMA(0,0,1) = MA(1)  |\n",
    "| 1 | 0 | 1 | ARIMA(1,0,1) = ARMA(1,1) |\n",
    "| 1 | 1 | 1 |      ARIMA(1,1,1)      |\n",
    "| 1 | 1 | 0 |      ARIMA(1,1,0)      |\n",
    "| 0 | 1 | 1 |      ARIMA(0,1,1)      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train-test split.\n",
    "y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARIMA model.\n",
    "from statsmodels.tsa.arima_model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual GridSearch\n",
    "\n",
    "Our strategy is this:\n",
    "- Let's start AIC at some really, really big number and call it `best_aic`.\n",
    "- Create variables `best_p` and `best_q` to store our best values of $p$ and $q$.\n",
    "- Create a nested `for` loop to iterate over our possible values of $p$ and $q$.\n",
    "    - At every combination of $p$ and $q$, fit an ARIMA model.\n",
    "    - If the ARIMA model has a better (lower) AIC than `best_aic`, then let's overwrite `best_aic`, `best_p`, and `best_q` with the values of AIC, $p$, and $q$ that our better model is using!\n",
    "    - If the ARIMA model has a worse (higher) AIC than `best_aic`, do nothing.\n",
    "- At the end of the `for` loop, tell us which values of $p$ and $q$ give us the best model (lowest AIC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What should our main concerns be about a manual GridSearch process?</summary>\n",
    "\n",
    "- The amount of time it takes to fit our models!\n",
    "- Cross-validation is also more complicated with time series data; you can check out more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) and [here](https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What is the purpose of try and except statements?</summary>\n",
    "\n",
    "- If our code encounters an error, then it will automatically stop. Sometimes this is good (so we can debug) but sometimes this isn't desirable!\n",
    "- `try` and `except` statements allow us to \"try\" to do something and, if there's an error, we can just `pass` it so that our code doesn't stop running.\n",
    "- This isn't always a good thing... errors are usually telling us that something is wrong. But we're going to hack our answer here so that we can check all values of $p$ and $q$ and just not run the model if some values of $p$ and $q$ are invalid.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting AIC, p, and q.\n",
    "best_aic =\n",
    "best_p =\n",
    "best_q =\n",
    "\n",
    "# Use nested for loop to iterate over values of p and q.\n",
    "\n",
    "\n",
    "        \n",
    "        # Insert try and except statements.\n",
    "        try:\n",
    "            \n",
    "            # Fitting an ARIMA(p, 1, q) model.\n",
    "            print(f'')\n",
    "            \n",
    "            # Instantiate ARIMA model.\n",
    "            arima = \n",
    "            \n",
    "            \n",
    "            # Fit ARIMA model.\n",
    "            model =\n",
    "\n",
    "            # Print out AIC for ARIMA(p, 1, q) model.\n",
    "            print(f'The AIC for ARIMA({p},1,{q}) is: {}')\n",
    "\n",
    "            # Is my current model's AIC better than our best_aic?\n",
    "            if\n",
    "                \n",
    "                # If so, let's overwrite best_aic, best_p, and best_q.\n",
    "                best_aic =\n",
    "                best_p =\n",
    "                best_q =\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "print()\n",
    "print()\n",
    "print('MODEL FINISHED!')\n",
    "print(f'Our model that minimizes AIC on the training data is the ARIMA({best_p},1,{best_q}).')\n",
    "print(f'This model has an AIC of {best_aic}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finding coefficients for an MA model is difficult, because the model requires us to regress on errors, but we don't actually observe these errors. The algorithm for fitting this is complicated and is well beyond the scope of this lesson. [Check more out here](https://www.it.uu.se/research/publications/reports/2006-022/2006-022-nc.pdf). We're using `try` and `except` statements so Python errors don't cause us to stop our model-fitting process!\n",
    "\n",
    "The model we want to fit is an $ARIMA(1,1,3)$ model. That would look like this:\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "Y^{(1)}_t &=& AR(1) + MA(3) \\\\\n",
    "(Y_t - Y_{t-1}) &=& \\beta_0 + \\beta_1(Y_{t-1} - Y_{t-2}) + w_1\\varepsilon_{t-1} + w_2\\varepsilon_{t-2} + w_3\\varepsilon_{t-3} + \\varepsilon_t \\\\\n",
    "\\end{eqnarray*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate best model.\n",
    "model =\n",
    "\n",
    "\n",
    "# Fit ARIMA model.\n",
    "arima =\n",
    "\n",
    "# Generate predictions based on test set.\n",
    "preds =\n",
    "\n",
    "# Plot data.\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Plot training data.\n",
    "plt.plot(y_train.index, pd.DataFrame(y_train).diff(), color = 'blue')\n",
    "\n",
    "# Plot testing data.\n",
    "plt.plot(y_test.index, pd.DataFrame(y_test).diff(), color = 'orange')\n",
    "\n",
    "# Plot predicted test values.\n",
    "plt.plot(y_test.index, preds, color = 'green')\n",
    "\n",
    "plt.title(label = 'Once-Differenced Global Mean Temperature with ARIMA(1, 1, 3) Predictions', fontsize=16)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What do these predicted values represent?</summary>\n",
    "\n",
    "- These predicted values represent $Y'_t$, which is the once-differenced mean temperature data.\n",
    "- The first observation represents the \"change in average temperature\" in 1752.\n",
    "- The second observation represents the \"change in average temperature\" in 1753.\n",
    "- And so on.\n",
    "- If you want to \"undo\" the predicted values to forecast forward, you can take the original $Y$ value at time $t-1$ and add that to $Y'_t$.\n",
    "\n",
    "$$Y'_t = Y_t - Y_{t-1} \\Rightarrow Y_t = Y'_t + Y_{t-1}$$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Note about SARIMA or SARIMAX\n",
    "---\n",
    "With ARIMA we've captured trend and difference, but what about seasonality? None of the methods we've talked about yet include it. \n",
    "\n",
    "Enter seasonal ARIMA (or SARIMA). (The 'X' at the end is a python thing that indicates the model also supports exogenous variables.) It takes all the parameters of the base ARIMA model, with additional parameters (denoted $P, D, Q$ and $m$) to account for lags in season.\n",
    "\n",
    "So we denote the whole thing as: $SARIMA(p,d,q)(P,D,Q)m$ where $m$ indicates the number of timesteps in a season.\n",
    "\n",
    "A P=1 would make use of the first seasonally offset observation in the model, e.g. $t-(m*1)$ or $t-12$. A P=2, would use the last two seasonally offset observations $t-(m * 1)$, $t-(m * 2)$.\n",
    "\n",
    "Similarly, a D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order errors in the model (e.g. moving average).\n",
    "\n",
    "More information about SARIMA can, of course, be found [in the documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html).\n",
    "\n",
    "And [this tutorial](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/) from Dr. Jason Brownlee has a good overview of how to implement SARIMA in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "- ARIMA models are among the most common approaches to time series modeling.\n",
    "- Highly flexible; it can model time series with varying characteristics.\n",
    "    - It takes information from both long-term trends and sudden shocks!\n",
    "- Can easily be extended into more advanced models.\n",
    "    - Seasonal ARIMA.\n",
    "    - ARIMA with eXogenous Predictors. (Independent X variables.)\n",
    "    - Vector ARIMA models. (Multiple Y variables simultaneously.)\n",
    "- Tends to perform well with moderate amounts of data, but may be outperformed by deep learning methods (RNN) with lots of data. (Side note: It can be hard to get lots of time series data!)\n",
    "- ARIMA models are best suited for short-term forecasts, but very quickly will start predicting the mean. Some of the extensions to ARIMA models can handle this better.\n",
    "\n",
    "**Forecasting time series is exceptionally difficult to do well!** We're using the past to predict the future, and there aren't guarantees that the future is necessarily going to be reflective of the past. There's more uncertainty in time series than we'd see in \"traditional\" model-building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are examples of time series data?</summary>\n",
    "\n",
    "_(Answers will vary.)_\n",
    "- Any data that is gathered over time is considered time series data.\n",
    "- Examples include an individual's maximum heart rate per minute, average temperature per hour, number of units sold per day, amount of rainfall per month, and GDP per quarter.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What are some disadvantages to time series analysis methods?</summary>\n",
    "\n",
    "_(Answers will vary.)_\n",
    "- Forecasting into the future is difficult! There are more sources of uncertainty than in traditional, non-temporal machine learning.\n",
    "- The amount of data gathered over time is often smaller than the amount of data in non-temporal machine learning.\n",
    "- Forecasts may only be meaningful/informative for a few time periods.\n",
    "- [Significant changes](gov/pmc/articles/PMC5464762/) may cause certain periods of data to be irrelevant/less relevant to solving problem. (e.g. data prior to Great Recession may not be impactful when forecasting economic data after 2009, data prior to COVID may not be meaningful when predicting consumer behavior after May 2020.)\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
